```markdown
# NYC Taxi ETL Pipeline ğŸš•âœ¨

This project is an end-to-end **ETL pipeline** for NYC Taxi trip data:
- Ingests Parquet data
- Cleans and filters rows using **Apache Spark**
- Loads cleaned data into a **Snowflake** data warehouse
- Orchestrated by **Apache Airflow**, running in **Docker**

---

## ğŸš€ Tech Stack

- **Apache Spark**: Data cleaning & transformation
- **Snowflake**: Cloud data warehouse
- **Apache Airflow**: Workflow orchestration
- **Docker Compose**: Local dev & deployment

---

## ğŸ“‚ Project Structure

```

.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ nyc\_taxi\_etl\_dag.py     # Airflow DAG calling spark-submit
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etl\_spark.py            # Standalone Spark ETL
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Input Parquet files
â”‚   â””â”€â”€ processed/              # Cleaned Parquet output
â”œâ”€â”€ airflow/                    # Airflow metadata DB & logs (local only)
â”œâ”€â”€ docker-compose.yml          # Airflow setup in Docker
â”œâ”€â”€ .env                        # â— NOT committed â€” contains Snowflake creds
â””â”€â”€ README.md

```

---

## âš™ï¸ How It Works

1. **Spark ETL**
   - Reads raw NYC Yellow Taxi data (Parquet)
   - Filters out invalid rows (e.g., negative fare/passenger count)
   - Writes cleaned data as single-part Parquet
   - Uses `.env` for Snowflake creds

2. **Load to Snowflake**
   - Uses `PUT` and `COPY INTO` to stage & load cleaned data
   - Loads into `nyc_taxi_cleaned` table
   - Validates row counts

3. **Airflow Orchestration**
   - `nyc_taxi_etl_dag` uses `BashOperator`:
     - Runs Spark job with `spark-submit`
   - Uses environment variables from `.env`
   - All containers run via Docker Compose

---

## ğŸ—ï¸ Environment Variables

Create a `.env` in your project root:
```

SNOWFLAKE\_USER=your\_username
SNOWFLAKE\_PASSWORD=your\_password
SNOWFLAKE\_ACCOUNT=your\_account
SNOWFLAKE\_WAREHOUSE=your\_warehouse
SNOWFLAKE\_DATABASE=your\_database
SNOWFLAKE\_SCHEMA=your\_schema

````
â— **Never commit this file!** Add `.env` to `.gitignore`.

---

## ğŸ³ Quick Start

```bash
# 1ï¸âƒ£ Clone this repo
git clone https://github.com/YOUR_GITHUB_USERNAME/nyc-taxi-etl.git
cd nyc-taxi-etl

# 2ï¸âƒ£ Create .env file (see above)

# 3ï¸âƒ£ Initialize Airflow DB & admin user
docker compose up airflow-init

# 4ï¸âƒ£ Start Airflow webserver & scheduler
docker compose up -d

# 5ï¸âƒ£ Access Airflow UI:
http://localhost:8080
username: admin
password: admin

# 6ï¸âƒ£ Trigger the DAG & watch logs!
````

---

## âœ… Next Improvements

* [ ] Use Airflow Variables & Connections for secrets
* [ ] Store raw files in S3 and use external stage
* [ ] Add Slack/Email failure notifications
* [ ] Schedule DAG to run daily
* [ ] Deploy Spark on a real cluster

---

## âœ¨ Credits

Created as a practical data engineering project to demonstrate ETL orchestration, big data processing, and cloud data warehousing.

---

**ğŸ“¬ Questions? PRs welcome!**

````

---

## âœ…âœ…âœ… HOW TO PUSH TO GITHUB

1ï¸âƒ£ **Initialize git** (if you havenâ€™t):
```bash
git init
````

2ï¸âƒ£ **Add remote** (if you havenâ€™t):

```bash
git remote add origin https://github.com/YOUR_USERNAME/nyc-taxi-etl.git
```

3ï¸âƒ£ **Stage everything**:

```bash
git add .
```

4ï¸âƒ£ **Double check your `.gitignore` includes `.env`!**

```bash
# Always test:
git status
```

You should NOT see `.env` as staged.

5ï¸âƒ£ **Commit your changes**:

```bash
git commit -m "Initial commit: NYC Taxi ETL with Spark, Airflow, Snowflake, Docker"
```

6ï¸âƒ£ **Push to GitHub**:

```bash
git branch -M main  # optional
git push -u origin main
```


