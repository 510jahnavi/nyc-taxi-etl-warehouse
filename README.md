# NYC Taxi ETL Pipeline ğŸš•âœ¨

A complete end-to-end ETL pipeline using **Apache Airflow**, **PySpark**, and **Snowflake**, containerized with **Docker Compose**.

---
## ğŸš€ Overview

- **Input**: Raw NYC Yellow Taxi `.parquet` data
- **ETL**: Data cleaning with PySpark
- **Orchestration**: DAGs using Apache Airflow
- **Warehouse**: Final data loaded into Snowflake
- **Containerization**: All services run in Docker

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ dags/                  # Airflow DAGs
â”‚   â””â”€â”€ nyc_taxi_etl_dag.py
â”œâ”€â”€ scripts/               # Standalone Spark ETL script
â”‚   â””â”€â”€ etl_spark.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Raw Parquet files
â”‚   â””â”€â”€ processed/         # Cleaned Parquet output
â”œâ”€â”€ airflow/               # Airflow metadata DB & logs (local only)
â”œâ”€â”€ docker-compose.yml     # Docker Compose config
â”œâ”€â”€ .env                   # â— NOT committed â€” contains Snowflake creds
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/your-username/nyc-taxi-etl-warehouse.git
cd nyc-taxi-etl-warehouse
```

### 2. Create `.env` file for Snowflake credentials
```bash
touch .env
```

Paste this inside `.env` (replace with your credentials):
```env
SNOWFLAKE_USER=MISJAHNAVI510
SNOWFLAKE_PASSWORD=YourPassword
SNOWFLAKE_ACCOUNT=DHXZDRX-ZI36614
SNOWFLAKE_WAREHOUSE=MY_WH
SNOWFLAKE_DATABASE=NYC_TAXI_DB
SNOWFLAKE_SCHEMA=NYC_TAXI_SCHEMA
```

> âœ… `.env` is **ignored** by Git via `.gitignore`

---

### 3. Add Raw Data

Place at least one `.parquet` file in:
```
data/raw/yellow_tripdata_2024-12.parquet
```

You can download sample data from: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

---

### 4. Start the Airflow stack
```bash
docker compose up --build
```

> Access Airflow at: http://localhost:8080  
> Default login: `admin / admin`

---

### 5. Trigger the DAG

- Open the **Airflow UI**
- Toggle the `nyc_taxi_etl_dag` to **"On"**
- Click â–¶ï¸ to trigger the DAG manually

---

## ğŸ§  DAG Flow

1. **PySpark ETL**:  
   - Reads `.parquet` files from `data/raw/`
   - Cleans data: removes rows with 0 passengers or fare
   - Writes cleaned data to `data/processed/` as `.parquet`

2. **Snowflake Load**:  
   - Reads latest `.parquet` file
   - Uploads to Snowflake stage
   - Loads into `nyc_taxi_cleaned` table via `COPY INTO`

---

## ğŸ Run Spark script locally (optional)
```bash
spark-submit scripts/etl_spark.py
```

---

## ğŸ§¾ Sample Output

Logs show counts:
```
âœ… RAW DF COUNT: 6131
âœ… CLEAN DF COUNT: 5678
âœ… Final rows in nyc_taxi_cleaned: 5678
```

---

## ğŸ§¹ To stop & clean up
```bash
docker compose down --volumes
```

---

## ğŸ”’ Security

- Snowflake credentials are stored in `.env` file (never committed).
- Output `.parquet` files are verified for size (>50KB) before uploading.
- Errors raise exceptions and prevent partial uploads.

---

## ğŸ” Pushing to GitHub

```bash
git add .
git commit -m "Initial ETL pipeline with Airflow, Spark & Snowflake"
git remote add origin https://github.com/your-username/nyc-taxi-etl-warehouse.git
git push -u origin main
```

---

## ğŸ“œ License

MIT Â© 2025
